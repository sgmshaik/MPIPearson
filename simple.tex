\title{Assignment 1 : Computing the Pearson correlation coefficient}
\author{
        Mousa Shaikh-Soltan \\
                Student ID : 201292277
}
\date{\today}

\documentclass[12pt]{article}

\begin{document}
\maketitle


\section*{Problem Overview}
\paragraph{}
This assignment involves writing and analysing a serial 
and MPI-parallel c program to solve the Pearson coefficient
of two $\sin(x)$ functions $\sin(i)$ and $\sin(i+5)$ .      
In order to investigate to possible MPI speed ups gained when using large arrays with 
minimum size of $2*10^6$;


\section{Overview of Implementation}
\paragraph{}
The serial implementation is straightforward with a functional c implementation used to separate the initialisation , mean , standard deviation ($\sigma$) and numerator of the Pearson Coefficient ($P_{xy}$). This also allows us to re-use the functions in the MPI implementation. 
The arrays are created dynamically using malloc allowing the user to input the size at the command line .  
\paragraph*{}
The parallel implementation uses a scatter and reduce approach due to it's intuitiveness and the added bonus that the inbuilt MPI methods are already optimised reducing some of the blocking/memory usage that may occur with an alternative send receive broadcast and reduce approach .
The \textbf{MPI\textunderscore Scatterv} is used to distrubute the data amoung the processes. We use \textbf{MPI\textunderscore Scatterv} instead of \textbf{MPI\textunderscore Scatter} because of it's can distribute buffers of different sizes. This happens when there is a remainder when dividing the size of the array between the number of set processes . We then use 
\textbf{MPI\textunderscore Allreduce} to sum all the local means which are calculated at each rank using the mean function applied to the local recieve buffer . \textbf{Allreduce} is used because the mean is required to be shared between all processes since it is required in $sigma$ and $P_{xy}$ numerator calculation [simpler to code and possibly more optimised than reduce $ -> $ then braodcast] . Now all the required data has been scattered and and the mean is available to all processes we callculate the local $sigma$ and local $P_{xy}$ numerators then use \textbf{MPI\textunderscore reduce} to MPIsum and send back to rank 0 as a single double . The rest of the calclulation is performad on rank 0; The timing is done as shown in the example class using MPi\textunderscore Wtime , starting in rank O after the initialisation and ending in rank 0 after the $P_xy$ has been calculated returning the elapsed time in seconds . 

\section{Results}




\section{Conclusions}\label{conclusions}


%\bibliographystyle{abbrv}

%\bibliography{simple}
\end{document}


